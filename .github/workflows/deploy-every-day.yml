name: WCAG Accessibility Testing (GitHub runner) - Daily
on:
  schedule:
    - cron: '0 0 * * *'          # every day at 00:00 UTC
  workflow_dispatch:            # manual run always allowed

permissions:
  contents: write
  pages: write
  id-token: write
  actions: write

jobs:
  # ──────────────────────────────────────────────────────────
  # Setup job (no gate-keeper needed for daily runs)
  # ──────────────────────────────────────────────────────────
  setup:
    runs-on: ubuntu-24.04
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
      cache-path: ${{ steps.cache-output.outputs.cache-path }}
    steps:
      - uses: actions/checkout@v5
      - uses: actions/setup-node@v5
        with:
          node-version: '22'
      - name: Cache Node.js modules
        uses: actions/cache@v4
        with:
          path: ~/.npm
          key: ${{ runner.os }}-node-${{ hashFiles('**/package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-node-
      - name: Cache Playwright browser directory
        id: playwright-cache
        uses: actions/cache@v4
        with:
          path: ~/.cache/ms-playwright
          key: ${{ runner.os }}-playwright-${{ hashFiles('**/package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-playwright-
      - name: Install dependencies and Playwright browsers
        id: cache-output
        run: |
          npm ci
          npx playwright install chromium firefox webkit --with-deps
          echo "cache-path=$(pwd)" >> $GITHUB_ENV
      - name: Generate test distribution
        id: set-matrix
        run: |
          SPECS=(tests/*.spec.ts)
          TOTAL_SPECS=${#SPECS[@]}
          BATCH_SIZE=$(( TOTAL_SPECS / 2 ))
          if [ "$BATCH_SIZE" -lt 1 ]; then
            BATCH_SIZE=1
          fi
          BATCH_COUNT=2
          BATCHES=$(seq -s ',' 0 $((BATCH_COUNT - 1)) | jq -R 'split(",")' | jq -c '{batch: .}')
          echo "matrix=$BATCHES" >> $GITHUB_OUTPUT
          echo "Total specs: $TOTAL_SPECS"
          echo "Batch size: $BATCH_SIZE"
          echo "Batch count: $BATCH_COUNT"

  wcag-test:
    needs: setup
    runs-on: ubuntu-24.04
    strategy:
      matrix: ${{ fromJson(needs.setup.outputs.matrix) }}
      fail-fast: false
      max-parallel: 2
    steps:
      - name: Checkout repository
        uses: actions/checkout@v5
        with:
          fetch-depth: 0
      - uses: actions/setup-node@v5
        with:
          node-version: '22'
      - name: Cache Node.js modules
        uses: actions/cache@v4
        with:
          path: ~/.npm
          key: ${{ runner.os }}-node-${{ hashFiles('**/package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-node-
      - name: Cache Playwright browser directory
        uses: actions/cache@v4
        with:
          path: ~/.cache/ms-playwright
          key: ${{ runner.os }}-playwright-${{ hashFiles('**/package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-playwright-
      - name: Install Node.js dependencies
        run: npm ci
      - name: Install Playwright browsers with dependencies
        run: |
          export PLAYWRIGHT_BROWSERS_PATH=~/.cache/ms-playwright
          npx playwright install chromium firefox webkit --with-deps
      - name: Ensure Python and venv are available
        run: |
          # Check if python3 and venv are available
          if ! python3 -m venv --help >/dev/null 2>&1; then
            echo "Warning: Python venv module not available. Installing python3-venv..."
            sudo apt-get update && sudo apt-get install -y python3-venv
          else
            echo "Python venv module is available"
          fi
      - name: Set up Python virtual environment
        run: |
          python3 -m venv venv
          source venv/bin/activate
          pip install --upgrade pip
          pip install beautifulsoup4 requests
      - name: Generate projects.json file
        run: |
          source venv/bin/activate
          python3 crawler.py
          # Copy any invalid_urls files to artifacts for later use
          if [ -d "historical-data/invalid-urls" ]; then
            mkdir -p invalid-urls-artifacts
            cp historical-data/invalid-urls/invalid_urls_*.json invalid-urls-artifacts/ 2>/dev/null || echo "No invalid URLs files found"
          fi
      - name: Generate spec files
        run: |
          source venv/bin/activate
          python3 generate_specs.py
          # Copy tag-summary.js and validation script to tests directory
          cp tag-summary.js tests/
          cp validate-summaries.js tests/
          cp merge-summary-parts.js tests/
      - name: Run WCAG Accessibility Tests for batch
        env:
          NODE_OPTIONS: "--max_old_space_size=4096"
          BATCH_INDEX: ${{ matrix.batch }}
        run: |
          export BATCH_INDEX=${{ matrix.batch }}
          mapfile -t ALL_SPECS < <(ls tests/*.spec.ts)
          TOTAL_SPECS=${#ALL_SPECS[@]}
          BATCH_SIZE=$(( TOTAL_SPECS / 2 ))
          if [ "$BATCH_SIZE" -lt 1 ]; then
            BATCH_SIZE=1
          fi
          START_INDEX=$((${{ matrix.batch }} * $BATCH_SIZE))
          for ((i = START_INDEX; i < START_INDEX + BATCH_SIZE && i < ${#ALL_SPECS[@]}; i++)); do
            if [ -f "${ALL_SPECS[$i]}" ]; then
              SPEC_FILE="${ALL_SPECS[$i]}"
              PROJECT_NAME=$(basename "$SPEC_FILE" .spec.ts | tr '_' ' ')
              echo "Running Playwright tests for '$PROJECT_NAME' ($SPEC_FILE)"
              npx playwright test "$SPEC_FILE" || echo "Test completed with violations for $PROJECT_NAME"
            fi
          done
        continue-on-error: true
      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-batch-${{ matrix.batch }}
          path: |
            accessibility-reports/
            playwright-report/
          retention-days: 30

  merge-summaries:
    needs: wcag-test
    runs-on: ubuntu-24.04
    if: always()
    steps:
      - name: Checkout repository
        uses: actions/checkout@v5
        with:
          fetch-depth: 0
          
      - name: Setup Node.js
        uses: actions/setup-node@v5
        with:
          node-version: '22'
          
      - name: Download all test artifacts
        uses: actions/download-artifact@v5
        with:
          path: artifacts
          pattern: test-results-batch-*
          merge-multiple: true
          
      - name: Merge Summary Parts
        run: |
          echo "Merging summary part files..."
          
          # Get today's date
          TODAY=$(date +%Y-%m-%d)
          echo "Merging summaries for date: $TODAY"
          
          # Create target directory structure
          mkdir -p "accessibility-reports/summaries/$TODAY"
          
          # Copy all part files from artifacts
          find artifacts -name "daily_summary_part_*.json" -exec cp {} "accessibility-reports/summaries/$TODAY/" \;
          
          # List found part files
          echo "Found part files:"
          ls -la "accessibility-reports/summaries/$TODAY/daily_summary_part_"*.json 2>/dev/null || echo "No part files found"
          
          # Run merge (2 is the number of parallel jobs/batches)
          node merge-summary-parts.js "$TODAY" 2
          
          # Verify the merged file
          if [ -f "accessibility-reports/summaries/$TODAY/daily_summary.json" ]; then
            echo "✓ Successfully created merged summary file"
            echo "File size: $(wc -c < "accessibility-reports/summaries/$TODAY/daily_summary.json") bytes"
            
            # Show summary stats
            node -e "
              const fs = require('fs');
              const data = JSON.parse(fs.readFileSync('accessibility-reports/summaries/$TODAY/daily_summary.json', 'utf8'));
              console.log('Summary stats:');
              console.log('- Tags:', data.tags?.length || 0);
              console.log('- Rule IDs:', data.rule_ids?.length || 0);
              const totalUrls = [...(data.tags || []), ...(data.rule_ids || [])].reduce((sum, item) => sum + (item.urls?.length || 0), 0);
              console.log('- Total URL references:', totalUrls);
            "
          else
            echo "✗ Failed to create merged summary file"
            exit 1
          fi
          
      - name: Upload Merged Summary
        uses: actions/upload-artifact@v4
        with:
          name: merged-summary-${{ github.run_id }}
          path: accessibility-reports/summaries/
          retention-days: 30

  deploy:
    needs: [wcag-test, merge-summaries]
    runs-on: ubuntu-24.04
    if: always()
    steps:
      - name: Checkout repository
        uses: actions/checkout@v5
        with:
          fetch-depth: 0
      - name: Checkout gh-pages branch
        uses: actions/checkout@v5
        with:
          ref: gh-pages
          path: gh-pages-branch
      - name: Download all artifacts
        uses: actions/download-artifact@v5
        with:
          path: artifacts
          pattern: test-results-batch-*
          merge-multiple: true
      - name: Download merged summary artifacts
        uses: actions/download-artifact@v5
        with:
          name: merged-summary-${{ github.run_id }}
          path: merged-summaries
      - name: Download invalid URLs files
        uses: actions/download-artifact@v5
        with:
          name: invalid-urls-files
          path: invalid-urls-artifacts
        continue-on-error: true
      - name: Copy JSON Reports and Organize by Date
        run: |
          echo "Copying and organizing JSON reports to gh-pages branch..."
          mkdir -p gh-pages-branch/accessibility-reports
          
          # Copy violation reports
          find artifacts -name "violations-*.json" | while read -r file; do
            if [[ $file =~ violations-.*-([0-9]{4}-[0-9]{2}-[0-9]{2}) ]]; then
              DATE_DIR="${BASH_REMATCH[1]}"
              mkdir -p "gh-pages-branch/accessibility-reports/$DATE_DIR"
              cp "$file" "gh-pages-branch/accessibility-reports/$DATE_DIR/"
            else
              echo "Warning: Could not extract date from filename: $file"
              cp "$file" "gh-pages-branch/accessibility-reports/"
            fi
          done
          
          # Copy summary files with folder structure - use merged summaries
          if [ -d "merged-summaries" ]; then
            echo "Copying merged summary files..."
            find merged-summaries -name "*.json" | while read -r file; do
              if [[ $file =~ summaries/([0-9]{4}-[0-9]{2}-[0-9]{2})/(.*\.json) ]]; then
                DATE_DIR="${BASH_REMATCH[1]}"
                FILE_NAME="${BASH_REMATCH[2]}"
                DEST_DIR="gh-pages-branch/accessibility-reports/summaries/$DATE_DIR"
                mkdir -p "$DEST_DIR"
                cp "$file" "$DEST_DIR/"
                echo "Copied merged summary file to $DEST_DIR/$FILE_NAME"
              fi
            done
          else
            echo "No merged summaries found, copying individual part files..."
            find artifacts -path "*/accessibility-reports/summaries/*" -name "*.json" | while read -r file; do
              if [[ $file =~ accessibility-reports/summaries/([0-9]{4}-[0-9]{2}-[0-9]{2})/(.*\.json) ]]; then
                DATE_DIR="${BASH_REMATCH[1]}"
                FILE_NAME="${BASH_REMATCH[2]}"
                DEST_DIR="gh-pages-branch/accessibility-reports/summaries/$DATE_DIR"
                mkdir -p "$DEST_DIR"
                cp "$file" "$DEST_DIR/"
                echo "Copied summary file to $DEST_DIR/$FILE_NAME"
              fi
            done
          fi
          
          echo "Contents of gh-pages-branch/accessibility-reports:"
          ls -R gh-pages-branch/accessibility-reports || echo "No JSON reports found"
          
      - name: Check Archive Quality Before Creating
        id: quality-check
        run: |
          echo "Checking if new results are better than existing archive..."
          
          # Get today's date
          TODAY=$(date +%Y-%m-%d)
          echo "Checking quality for date: $TODAY"
          
          # Copy the quality checker script to current directory
          cp check-archive-quality.js .
          
          # Check if today's directory exists and has content
          NEW_RESULTS_DIR="gh-pages-branch/accessibility-reports/$TODAY"
          if [ ! -d "$NEW_RESULTS_DIR" ] || [ -z "$(find "$NEW_RESULTS_DIR" -name "violations-*.json" 2>/dev/null)" ]; then
            echo "No new results found for $TODAY, skipping quality check"
            echo "should_create_archive=true" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          # Construct URL for existing manifest
          EXISTING_MANIFEST_URL="https://raw.githubusercontent.com/turbolego/uustreak/refs/heads/gh-pages/accessibility-reports/archives/reports_$TODAY.manifest"
          
          # Run quality check
          if node check-archive-quality.js "$TODAY" "$NEW_RESULTS_DIR" "$EXISTING_MANIFEST_URL"; then
            echo "Quality check passed: New results are better or equal"
            echo "should_create_archive=true" >> $GITHUB_OUTPUT
          else
            echo "Quality check failed: Existing archive is better"
            echo "should_create_archive=false" >> $GITHUB_OUTPUT
          fi
        continue-on-error: true

      - name: Create Gzip Archives for JSON Reports
        if: steps.quality-check.outputs.should_create_archive == 'true'
        run: |
          echo "Creating gzip archives for JSON reports by date..."
          
          # Find all date directories in accessibility-reports
          find gh-pages-branch/accessibility-reports -maxdepth 1 -type d -name "[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]" | while read -r date_dir; do
            DATE=$(basename "$date_dir")
            echo "Creating gzip archive for date: $DATE"
            
            # Create temporary directory for organizing files
            TEMP_DIR="temp_$DATE"
            mkdir -p "$TEMP_DIR"
            
            # Copy all JSON files for this date to temp directory
            if [ -d "$date_dir" ]; then
              find "$date_dir" -name "*.json" -exec cp {} "$TEMP_DIR/" \;
              echo "Found $(find "$date_dir" -name "*.json" | wc -l) JSON files for $DATE"
            fi
            
            # Also include summary files for this date if they exist
            if [ -d "gh-pages-branch/accessibility-reports/summaries/$DATE" ]; then
              find "gh-pages-branch/accessibility-reports/summaries/$DATE" -name "*.json" -exec cp {} "$TEMP_DIR/" \;
              echo "Found $(find "gh-pages-branch/accessibility-reports/summaries/$DATE" -name "*.json" | wc -l) summary files for $DATE"
            fi
            
            # Create gzip archive if we have files
            if [ "$(find "$TEMP_DIR" -name "*.json" | wc -l)" -gt 0 ]; then
              # Create archive directory
              mkdir -p "gh-pages-branch/accessibility-reports/archives"
              
              # Create tar.gz archive containing all JSON files for this date
              # Use file list approach to avoid "Argument list too long" error
              cd "$TEMP_DIR"
              find . -name "*.json" > filelist.txt
              tar -czf "../gh-pages-branch/accessibility-reports/archives/reports_$DATE.tar.gz" -T filelist.txt
              cd ..
              
              # Get file size for logging
              ARCHIVE_SIZE=$(du -h "gh-pages-branch/accessibility-reports/archives/reports_$DATE.tar.gz" | cut -f1)
              echo "Created archive: reports_$DATE.tar.gz (Size: $ARCHIVE_SIZE)"
              
              # Create a manifest file with the list of files in the archive
              tar -tzf "gh-pages-branch/accessibility-reports/archives/reports_$DATE.tar.gz" | sort > "gh-pages-branch/accessibility-reports/archives/reports_$DATE.manifest"
              echo "Created manifest: reports_$DATE.manifest"
            else
              echo "No JSON files found for $DATE, skipping archive creation"
            fi
            
            # Clean up temp directory
            rm -rf "$TEMP_DIR"
          done
          
          # Create archive index with metadata
          echo "Creating archive index..."
          cd gh-pages-branch/accessibility-reports/archives
          ls -la *.tar.gz 2>/dev/null | while read -r permissions links owner group size month day time filename; do
            if [[ "$filename" =~ reports_([0-9]{4}-[0-9]{2}-[0-9]{2})\.tar\.gz ]]; then
              DATE="${BASH_REMATCH[1]}"
              FILE_COUNT=$(wc -l < "reports_$DATE.manifest" 2>/dev/null || echo "0")
              echo "{\"date\":\"$DATE\",\"filename\":\"$filename\",\"size\":\"$size\",\"fileCount\":$FILE_COUNT}"
            fi
          done | jq -s 'sort_by(.date) | reverse' > archive_index.json 2>/dev/null || echo "[]" > archive_index.json
          cd ../../..
          
          echo "Archive creation completed. Summary:"
          ls -la gh-pages-branch/accessibility-reports/archives/ || echo "No archives created"
          
      - name: Validate and Repair Summary Files
        run: |
          echo "Validating and repairing summary files..."
          # Install Node.js for the validation script
          if command -v node >/dev/null 2>&1; then
            # Ensure validation script is available (it should already be in the current directory)
            if [ ! -f "validate-summaries.js" ]; then
              echo "Error: validate-summaries.js not found in current directory"
              exit 1
            fi
            
            # Change to the directory containing summary files
            cd gh-pages-branch
            
            # Run validation
            node ../validate-summaries.js
            
            echo "Summary validation completed"
          else
            echo "Node.js not available, skipping validation"
          fi
      - name: Copy Invalid URLs Reports to Historical Data
        run: |
          echo "Copying invalid URLs reports to historical data..."
          mkdir -p gh-pages-branch/historical-data/invalid-urls
          
          if [ -d "invalid-urls-artifacts" ]; then
            find invalid-urls-artifacts -name "invalid_urls_*.json" 2>/dev/null | while read -r file; do
              if [ -f "$file" ]; then
                cp "$file" "gh-pages-branch/historical-data/invalid-urls/"
                echo "Copied $file to gh-pages-branch/historical-data/invalid-urls/"
              fi
            done
          else
            echo "No invalid URLs artifacts directory found"
          fi
      - name: Generate and Copy Playwright HTML Reports
        run: |
          echo "Generating Playwright HTML reports..."
          TIMESTAMP=$(date +"%Y-%m-%d_%H-%M-%S")
          TARGET_DIR="gh-pages-branch/historical-data/$TIMESTAMP"
          mkdir -p "$TARGET_DIR"

          # Loop through all report directories for each project batch
          for REPORT_DIR in artifacts/test-results-batch-*/playwright-report; do
            echo "Checking directory: $REPORT_DIR"
            if [ -d "$REPORT_DIR" ]; then
              PROJECT_NAME=$(basename "$(dirname "$REPORT_DIR")")  # Extract project name
              PROJECT_DIR="$TARGET_DIR/$PROJECT_NAME"
              mkdir -p "$PROJECT_DIR"

              # Ensure the main report file is copied correctly
              if [ -f "$REPORT_DIR/index.html" ]; then
                cp "$REPORT_DIR/index.html" "$PROJECT_DIR/index.html"
                echo "Saved Playwright report for $PROJECT_NAME in $PROJECT_DIR/index.html"
              else
                echo "Warning: No index.html found in $REPORT_DIR"
              fi
            fi
          done

          echo "All HTML reports saved in $TARGET_DIR"

      - name: Update Report List
        if: steps.quality-check.outputs.should_create_archive == 'true'
        run: |
          echo "Updating report list in gh-pages branch..."
          mkdir -p gh-pages-branch/historical-data
          find gh-pages-branch/accessibility-reports -type f -name "violations-*.json" ! -name "*-FAILED.json" -print0 | \
          while IFS= read -r -d '' file; do
            relative_path=${file#gh-pages-branch/}
            echo "$relative_path"
          done | sort | jq -R . | jq -s . > gh-pages-branch/historical-data/report-list.json
          
          echo "Report list contents:"
          cat gh-pages-branch/historical-data/report-list.json
          echo "Number of reports: $(jq length gh-pages-branch/historical-data/report-list.json)"
      - name: Deploy static files to GitHub Pages
        if: steps.quality-check.outputs.should_create_archive == 'true'
        run: |
          echo "Copying static files to gh-pages branch..."
          cp index.html gh-pages-branch/
          cp all.html gh-pages-branch/
          cp bar.html gh-pages-branch/
          cp favicon.ico gh-pages-branch/
          cp README.md gh-pages-branch/
          
          # Copy new JavaScript and CSS files
          cp loading.js gh-pages-branch/
          cp summary.js gh-pages-branch/
          cp styles.css gh-pages-branch/
          cp tag-summary.js gh-pages-branch/
          cp invalid_urls.js gh-pages-branch/
          cp merge-summary-parts.js gh-pages-branch/
          cp validate-summaries.js gh-pages-branch/
          cp gzip-utils.js gh-pages-branch/
          
          echo "Static files copied to gh-pages branch"
      - name: Deploy to GitHub Pages
        if: steps.quality-check.outputs.should_create_archive == 'true'
        uses: peaceiris/actions-gh-pages@v4
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: gh-pages-branch
          publish_branch: gh-pages
          keep_files: true

      - name: Log Skipped Deployment
        if: steps.quality-check.outputs.should_create_archive == 'false'
        run: |
          echo "⚠️ Deployment skipped: New results are of lower quality than existing archive"
          echo "This protects the existing high-quality data from being overwritten"
          echo "The workflow completed successfully but no changes were deployed"
      
      - name: Trigger Issue Creation
        if: steps.quality-check.outputs.should_create_archive == 'true'
        uses: actions/github-script@v8
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            await github.rest.actions.createWorkflowDispatch({
              owner: context.repo.owner,
              repo: context.repo.repo,
              workflow_id: 'create-update-issues.yml',
              ref: 'main'
            })