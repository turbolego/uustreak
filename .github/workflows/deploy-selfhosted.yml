# Self-hosted accessibility testing workflow with comprehensive failure handling
# This workflow has 4 critical jobs that must ALL succeed:
# 1. setup: Generates test matrix and validates URLs
# 2. wcag-test: Runs accessibility tests on all URLs
# 3. merge-summaries: Consolidates test results
# 4. deploy: Uploads results to web server
#
# Failure handling:
# - If ANY job fails, the workflow terminates and retries (max 3 attempts)
# - All jobs must complete successfully before proceeding to deployment
# - Exponential backoff between retry attempts (5min, 10min, 15min)
# - Final failure results in workflow termination with notification

name: Deploy to Self-hosted
on:
  schedule:
    - cron: '0 0 * * *'          # every day at 00:00 UTC
  workflow_dispatch:   # Allows manual execution
    inputs:
      retry_count:
        description: 'Retry attempt number (internal use)'
        required: false
        default: '1'
        type: string

concurrency:
  group: wcag-testing-${{ github.ref }}
  cancel-in-progress: false  # Don't cancel running tests

permissions:
  contents: write
  pages: write
  id-token: write
  actions: write

jobs:
  setup:
    runs-on: self-hosted
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
      cache-path: ${{ steps.cache-output.outputs.cache-path }}
    steps:
      - name: Log workflow attempt
        run: |
          RETRY_COUNT=${{ github.event.inputs.retry_count || '1' }}
          echo "üöÄ Starting WCAG Accessibility Testing workflow - Attempt $RETRY_COUNT"
          if [[ $RETRY_COUNT -gt 1 ]]; then
            echo "This is a retry attempt due to previous failures."
          fi
      - name: Clean workspace for setup
        run: |
          # Kill any hanging npm or node processes first
          pkill -f npm || echo "No npm processes to kill"
          pkill -f node || echo "No node processes to kill"
          
          # Wait for processes to fully terminate
          sleep 3
          
          # Force cleanup of problematic directories (try without sudo first)
          rm -rf node_modules || echo "Cleaned node_modules"
          rm -rf .git/index.lock || echo "Cleaned git lock file"

      - uses: actions/checkout@v5
        with:
          fetch-depth: 0
          clean: false
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Configure Git for authentication
        run: |
          git config --global url."https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/".insteadOf "https://github.com/"
          git config --global user.email "github-actions@github.com"
          git config --global user.name "GitHub Actions"

      - uses: actions/setup-node@v5
        with:
          node-version: '22'
          registry-url: 'https://registry.npmjs.org'

      - name: Install dependencies and Playwright browsers
        id: cache-output
        run: |
          echo "Completely removing npm cache for setup..."

          # Force removal of all npm-related directories and files
          rm -rf ~/.npm ~/.cache/npm ~/.config/npm /tmp/npm-* 2>/dev/null || echo "Cleaned npm directories"

          # Also clean any npm processes that might be holding locks
          pkill -f npm || echo "No npm processes to kill"

          # Wait a moment for filesystem to settle
          sleep 2

          # Create completely fresh npm environment
          export NPM_CONFIG_CACHE=/tmp/npm-cache-$(date +%s)
          export NPM_CONFIG_PREFIX=/tmp/npm-prefix-$(date +%s)
          mkdir -p "$NPM_CONFIG_CACHE" "$NPM_CONFIG_PREFIX"

          # Configure npm to use GitHub token for authentication
          echo "//npm.pkg.github.com/:_authToken=${{ secrets.GITHUB_TOKEN }}" > ~/.npmrc
          echo "@github:registry=https://npm.pkg.github.com" >> ~/.npmrc

          # Set npm to use temporary directories and disable cache
          npm config set cache "$NPM_CONFIG_CACHE"
          npm config set prefix "$NPM_CONFIG_PREFIX"
          npm config set registry https://registry.npmjs.org/
          npm config set fund false
          npm config set audit false

          # Retry logic for npm installation
          MAX_RETRIES=3
          RETRY_DELAY=5
          for ((i=1; i<=MAX_RETRIES; i++)); do
            echo "Attempt $i: Installing dependencies with fresh cache..."
            npm install --no-save --no-package-lock --prefer-offline=false --cache="$NPM_CONFIG_CACHE" && break || {
              echo "Attempt $i failed. Retrying in $RETRY_DELAY seconds..."
              sleep $RETRY_DELAY
            }
          done

          # Install Playwright without requiring root permissions
          export PLAYWRIGHT_BROWSERS_PATH=$HOME/.cache/ms-playwright
          export PLAYWRIGHT_SKIP_VALIDATE_HOST_REQUIREMENTS=1
          npx playwright install chromium firefox webkit
          echo "cache-path=$(pwd)" >> $GITHUB_ENV

      - name: Ensure Python and venv are available
        run: |
          # Check if python3 and venv are available
          if ! python3 -m venv --help >/dev/null 2>&1; then
            echo "Warning: Python venv module not available. Please ensure python3-venv is installed on the runner."
            echo "Manual installation command: sudo apt-get install -y python3-venv"
            exit 1
          else
            echo "Python venv module is available"
          fi

      - name: Set up Python virtual environment
        run: |
          python3 -m venv venv
          source venv/bin/activate
          pip install --upgrade pip
          pip install beautifulsoup4 requests

      - name: Generate projects.json file
        run: |
          source venv/bin/activate
          python3 crawler.py
          if [ -d "historical-data/invalid-urls" ]; then
            mkdir -p invalid-urls-artifacts
            cp historical-data/invalid-urls/invalid_urls_*.json invalid-urls-artifacts/ 2>/dev/null || echo "No invalid URLs files found"
          fi

      - name: Generate spec files
        run: |
          source venv/bin/activate
          python3 generate_specs.py
          cp tag-summary.js tests/

      - name: Ensure jq is available
        run: |
          # Check if jq is available
          if ! command -v jq >/dev/null 2>&1; then
            echo "Warning: jq is not available. Please ensure jq is installed on the runner."
            echo "Manual installation command: sudo apt-get install -y jq"
            exit 1
          else
            echo "jq is available"
          fi

      - name: Generate test distribution
        id: set-matrix
        run: |
          SPECS=(tests/*.spec.ts)
          TOTAL_SPECS=${#SPECS[@]}
          TARGET_TESTS_PER_BATCH=8
          BATCH_COUNT=$(( (TOTAL_SPECS + TARGET_TESTS_PER_BATCH - 1) / TARGET_TESTS_PER_BATCH ))

          # Use conservative parallelism for self-hosted runners
          # Cross-platform CPU core detection (Linux: nproc, macOS: sysctl)
          AVAILABLE_CORES=$(nproc 2>/dev/null || sysctl -n hw.ncpu 2>/dev/null || echo "4")
          MAX_PARALLEL=$((AVAILABLE_CORES / 4))
          if [ "$MAX_PARALLEL" -gt 4 ]; then
            MAX_PARALLEL=4
          elif [ "$MAX_PARALLEL" -lt 1 ]; then
            MAX_PARALLEL=1
          fi

          if [ "$BATCH_COUNT" -gt "$MAX_PARALLEL" ]; then
            BATCH_COUNT=$MAX_PARALLEL
          fi

          BATCH_SIZE=$(( (TOTAL_SPECS + BATCH_COUNT - 1) / BATCH_COUNT ))

          BATCHES=$(seq -s ',' 0 $((BATCH_COUNT - 1)) | jq -R 'split(",")' | jq -c '{batch: .}')
          echo "matrix=$BATCHES" >> $GITHUB_OUTPUT
          echo "Total specs: $TOTAL_SPECS"
          echo "Batch count: $BATCH_COUNT"
          echo "Batch size: $BATCH_SIZE"
          echo "Max parallel jobs will be: $BATCH_COUNT"
          echo "Tests per batch: approximately $TARGET_TESTS_PER_BATCH"

      - name: Upload test files
        uses: actions/upload-artifact@v4
        with:
          name: test-spec-files
          path: tests/
          retention-days: 1

      - name: Upload invalid URLs files
        uses: actions/upload-artifact@v4
        with:
          name: invalid-urls-files
          path: invalid-urls-artifacts/
          retention-days: 30
        continue-on-error: true

      - name: Validate setup outputs
        run: |
          echo "Validating setup job outputs..."
          MATRIX_OUTPUT='${{ steps.set-matrix.outputs.matrix }}'
          
          if [[ -z "$MATRIX_OUTPUT" || "$MATRIX_OUTPUT" == "null" ]]; then
            echo "‚ùå ERROR: Matrix output is empty or null"
            echo "Matrix output: $MATRIX_OUTPUT"
            exit 1
          fi
          
          # Check if matrix is valid JSON
          echo "$MATRIX_OUTPUT" | jq . > /dev/null
          if [[ $? -ne 0 ]]; then
            echo "‚ùå ERROR: Matrix output is not valid JSON"
            echo "Matrix output: $MATRIX_OUTPUT"
            exit 1
          fi
          
          echo "‚úÖ Setup validation successful"
          echo "Matrix: $MATRIX_OUTPUT"

  wcag-test:
    needs: setup
    runs-on: self-hosted
    timeout-minutes: 60
    strategy:
      matrix: ${{ fromJson(needs.setup.outputs.matrix) }}
      fail-fast: true  # Stop all jobs if one fails
      max-parallel: 4
    steps:
      - name: Clean workspace manually
        run: |
          # Kill any hanging npm processes first
          pkill -f npm || echo "No npm processes to kill"
          pkill -f node || echo "No node processes to kill"
          
          # Wait for processes to fully terminate
          sleep 3
          
          # Force cleanup of problematic directories (try without sudo first)
          if [ -d "node_modules" ]; then
            echo "Removing node_modules directory..."
            rm -rf node_modules || echo "Cleaned node_modules with fallback"
          fi
          
          # Clean any other problematic directories
          rm -rf accessibility-reports || echo "Cleaned accessibility-reports"
          rm -rf playwright-report || echo "Cleaned playwright-report"
          rm -rf tests || echo "Cleaned tests"
          rm -rf .git/index.lock || echo "Cleaned git lock file"
          
          # Create required directories
          mkdir -p accessibility-reports
          mkdir -p playwright-report

      - name: Checkout repository
        uses: actions/checkout@v5
        with:
          fetch-depth: 0
          clean: false

      - name: Download test files
        uses: actions/download-artifact@v5
        with:
          name: test-spec-files
          path: tests/

      - uses: actions/setup-node@v5
        with:
          node-version: '22'

      - name: Verify project structure
        run: |
          echo "=== Project Structure Check ==="
          echo "Current directory: $(pwd)"
          echo "Contents:"
          ls -la
          echo ""
          echo "package.json exists: $([ -f package.json ] && echo "YES" || echo "NO")"
          if [ -f package.json ]; then
            echo "package.json content:"
            cat package.json
          fi
          echo ""
          echo "Node and npm versions:"
          node --version
          npm --version
          echo ""
          echo "Current npm configuration:"
          npm config list

      - name: Debug system resources
        run: |
          echo "=== System Resources Before npm install ==="
          echo "Memory info:"
          # Cross-platform memory check (Linux: free, macOS: vm_stat)
          if command -v free >/dev/null 2>&1; then
            free -h
          else
            # macOS alternative
            vm_stat | perl -ne '/page size of (\d+)/ and $size=$1; /Pages\s+(\w+):\s+(\d+)/ and printf("%-16s % 16.2f Mi\n", "$1:", $2 * $size / 1048576);'
          fi
          echo "CPU info:"
          # Cross-platform CPU core count
          CORES=$(nproc 2>/dev/null || sysctl -n hw.ncpu 2>/dev/null || echo "unknown")
          echo "CPU cores: $CORES"
          echo "CPU load average:"
          uptime
          echo "Disk space:"
          df -h
          echo "Temp directory space:"
          df -h /tmp
          echo "Available inodes:"
          # Cross-platform inode check (Linux: df -i, macOS: df -i works but format differs)
          if df -i / >/dev/null 2>&1; then
            df -i /
          else
            echo "Inode information not available on this system"
          fi
          echo "Running processes:"
          ps aux | grep -E '(npm|node)' || echo "No npm/node processes running"
          echo "Network connectivity test:"
          ping -c 2 registry.npmjs.org || echo "npm registry not reachable"

      - name: Install dependencies
        env:
          NODE_OPTIONS: "--max_old_space_size=2048"
        run: |
          echo "=== Starting npm installation ==="
          
          # Clean npm cache and configuration first
          npm cache clean --force || echo "Cache clean failed, continuing..."
          
          # Set npm configuration for reliable installation
          npm config set fund false
          npm config set audit false
          npm config set progress false
          npm config set loglevel error
          npm config set registry https://registry.npmjs.org/
          
          # Try different approaches in order of preference
          echo "Attempt 1: Standard npm install..."
          if npm install --no-fund --no-audit --prefer-offline=false; then
            echo "‚úì Standard npm install successful"
          elif npm install --no-fund --no-audit --no-package-lock --prefer-offline=false; then
            echo "‚úì npm install without package-lock successful"
          elif npm ci --no-fund --no-audit; then
            echo "‚úì npm ci successful"
          else
            echo "‚ùå All npm installation methods failed"
            echo "Checking npm logs..."
            cat ~/.npm/_logs/*debug*.log 2>/dev/null | tail -20 || echo "No npm logs found"
            exit 1
          fi
          
          echo "=== npm installation completed ==="
          echo "Verifying installation..."
          npm list --depth=0 || echo "Some dependencies may be missing but continuing..."

      - name: Install Playwright and Use Cached Browsers
        env:
          NODE_OPTIONS: "--max_old_space_size=2048"
        run: |
          echo "Installing Playwright browsers..."
          
          # Set browser cache path
          export PLAYWRIGHT_BROWSERS_PATH=~/.cache/ms-playwright
          export PLAYWRIGHT_SKIP_VALIDATE_HOST_REQUIREMENTS=1
          
          # Create browsers directory if it doesn't exist
          mkdir -p "$PLAYWRIGHT_BROWSERS_PATH"
          
          # Install browsers with retries
          BROWSER_INSTALL_SUCCESS=false
          for attempt in 1 2 3; do
            echo "Browser installation attempt $attempt..."
            if npx playwright install chromium firefox webkit; then
              echo "‚úì Playwright browsers installed successfully on attempt $attempt"
              BROWSER_INSTALL_SUCCESS=true
              break
            else
              echo "‚ùå Browser installation attempt $attempt failed"
              if [ $attempt -lt 3 ]; then
                echo "Waiting 10 seconds before retry..."
                sleep 10
              fi
            fi
          done
          
          if [ "$BROWSER_INSTALL_SUCCESS" = false ]; then
            echo "‚ùå All browser installation attempts failed"
            exit 1
          fi
          
          # Verify browsers are installed
          echo "Verifying browser installations..."
          ls -la "$PLAYWRIGHT_BROWSERS_PATH" || echo "Browser cache directory listing failed"

      - name: Run WCAG Accessibility Tests for batch
        env:
          NODE_OPTIONS: "--max_old_space_size=4096"
          BATCH_INDEX: ${{ matrix.batch }}
          PREFER_HASH_URL: "true"
          PLAYWRIGHT_BROWSERS_PATH: ~/.cache/ms-playwright
          PLAYWRIGHT_SKIP_VALIDATE_HOST_REQUIREMENTS: "1"
        run: |
          echo "=== Starting WCAG tests for batch ${{ matrix.batch }} ==="
          
          # Verify test files exist
          if [ ! -d "tests" ] || [ -z "$(ls -A tests/ 2>/dev/null)" ]; then
            echo "‚ùå No test files found in tests/ directory"
            ls -la tests/ || echo "tests/ directory does not exist"
            exit 1
          fi
          
          # Bash 3.2 compatible array population (macOS default bash doesn't support mapfile)
          ALL_SPECS=()
          while IFS= read -r -d '' file; do
            ALL_SPECS+=("$file")
          done < <(find tests -maxdepth 1 -name "*.spec.ts" -print0 | sort -z)
          TOTAL_SPECS=${#ALL_SPECS[@]}

          echo "Found $TOTAL_SPECS test specification files"
          echo "Matrix batch: ${{ matrix.batch }}"

          TARGET_TESTS_PER_BATCH=8
          BATCH_COUNT=$(( (TOTAL_SPECS + TARGET_TESTS_PER_BATCH - 1) / TARGET_TESTS_PER_BATCH ))

          # Cross-platform CPU core detection (Linux: nproc, macOS: sysctl)
          AVAILABLE_CORES=$(nproc 2>/dev/null || sysctl -n hw.ncpu 2>/dev/null || echo "4")
          MAX_PARALLEL=$((AVAILABLE_CORES / 4))
          if [ "$MAX_PARALLEL" -gt 4 ]; then
            MAX_PARALLEL=4
          elif [ "$MAX_PARALLEL" -lt 1 ]; then
            MAX_PARALLEL=1
          fi

          if [ "$BATCH_COUNT" -gt "$MAX_PARALLEL" ]; then
            BATCH_COUNT=$MAX_PARALLEL
          fi

          BATCH_SIZE=$(( (TOTAL_SPECS + BATCH_COUNT - 1) / BATCH_COUNT ))
          START_INDEX=$((${{ matrix.batch }} * BATCH_SIZE))
          END_INDEX=$((START_INDEX + BATCH_SIZE))

          echo "Processing batch ${{ matrix.batch }} with tests $START_INDEX to $((END_INDEX-1))"
          echo "Batch size: $BATCH_SIZE, Total batches: $BATCH_COUNT"
          
          # Create output directories
          mkdir -p accessibility-reports playwright-report
          
          TESTS_RUN=0
          TESTS_FAILED=0
          
          for ((i = START_INDEX; i < END_INDEX && i < ${#ALL_SPECS[@]}; i++)); do
            if [ -f "${ALL_SPECS[$i]}" ]; then
              SPEC_FILE="${ALL_SPECS[$i]}"
              PROJECT_NAME=$(basename "$SPEC_FILE" .spec.ts | tr '_' ' ')
              echo ""
              echo "üß™ Running tests for '$PROJECT_NAME' ($SPEC_FILE)..."
              
              TESTS_RUN=$((TESTS_RUN + 1))
              
              if npx playwright test "$SPEC_FILE"; then
                echo "‚úÖ Tests completed successfully for $PROJECT_NAME"
              else
                echo "‚ö†Ô∏è  Tests completed with issues for $PROJECT_NAME (this is expected for accessibility violations)"
                TESTS_FAILED=$((TESTS_FAILED + 1))
              fi
            fi
          done
          
          echo ""
          echo "=== Batch ${{ matrix.batch }} Summary ==="
          echo "Tests run: $TESTS_RUN"
          echo "Tests with issues: $TESTS_FAILED"
          echo "Output files generated:"
          find accessibility-reports -name "*.json" 2>/dev/null | wc -l | xargs echo "  Accessibility reports:"
          find playwright-report -name "*.html" 2>/dev/null | wc -l | xargs echo "  Playwright reports:"
        continue-on-error: true

      - name: Upload test results
        if: always()
        run: |
          # Create artifact directories and upload only if they exist with content
          mkdir -p test-results-batch-${{ matrix.batch }}
          
          if [ -d "accessibility-reports" ] && [ "$(ls -A accessibility-reports 2>/dev/null)" ]; then
            echo "Copying accessibility reports..."
            cp -r accessibility-reports test-results-batch-${{ matrix.batch }}/
          else
            echo "No accessibility reports found, creating empty directory"
            mkdir -p test-results-batch-${{ matrix.batch }}/accessibility-reports
          fi
          
          if [ -d "playwright-report" ] && [ "$(ls -A playwright-report 2>/dev/null)" ]; then
            echo "Copying playwright reports..."
            cp -r playwright-report test-results-batch-${{ matrix.batch }}/
          else
            echo "No playwright reports found, creating empty directory"
            mkdir -p test-results-batch-${{ matrix.batch }}/playwright-report
          fi

      - name: Upload artifacts for batch
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-batch-${{ matrix.batch }}
          path: test-results-batch-${{ matrix.batch }}/
          retention-days: 30

  merge-summaries:
    needs: wcag-test
    runs-on: self-hosted
    if: success() # Only run if wcag-test succeeded
    steps:
      - name: Checkout repository
        uses: actions/checkout@v5
        with:
          fetch-depth: 0
          
      - name: Setup Node.js
        uses: actions/setup-node@v5
        with:
          node-version: '22'
          
      - name: Download all test artifacts
        uses: actions/download-artifact@v5
        with:
          path: artifacts
          pattern: test-results-batch-*
          merge-multiple: true
          
      - name: Merge Summary Parts
        run: |
          echo "Merging summary part files..."
          
          # Get today's date
          TODAY=$(date +%Y-%m-%d)
          echo "Merging summaries for date: $TODAY"
          
          # Create target directory structure
          mkdir -p "accessibility-reports/summaries/$TODAY"
          
          # Copy all part files from artifacts
          find artifacts -name "daily_summary_part_*.json" -exec cp {} "accessibility-reports/summaries/$TODAY/" \;
          
          # Debug: Show structure of artifacts directory
          echo "=== DEBUG: Artifacts directory structure ==="
          find artifacts -type f -name "*.json" | head -20
          echo "=== DEBUG: Looking for summary files specifically ==="
          find artifacts -name "*summary*.json"
          
          # List found part files
          echo "Found part files:"
          ls -la "accessibility-reports/summaries/$TODAY/daily_summary_part_"*.json 2>/dev/null || echo "No part files found"
          
          # Calculate the number of parts based on the number of batches
          PARTS_COUNT=$(find artifacts -name "daily_summary_part_*.json" | wc -l)
          echo "Found $PARTS_COUNT summary parts to merge"
          
          # Run merge using the actual number of parts found
          if [ "$PARTS_COUNT" -gt 0 ]; then
            node merge-summary-parts.js "$TODAY" "$PARTS_COUNT"
            
            # Verify the merged file
            if [ -f "accessibility-reports/summaries/$TODAY/daily_summary.json" ]; then
              echo "‚úì Successfully created merged summary file"
              echo "File size: $(wc -c < "accessibility-reports/summaries/$TODAY/daily_summary.json") bytes"
            else
              echo "‚úó Failed to create merged summary file"
            fi
          else
            echo "No summary parts found to merge"
          fi
          
      - name: Upload merged summary
        uses: actions/upload-artifact@v4
        with:
          name: merged-summary
          path: accessibility-reports/summaries/
          retention-days: 30

  check-job-status:
    needs: [setup, wcag-test, merge-summaries]
    runs-on: self-hosted
    if: always()
    outputs:
      all_jobs_successful: ${{ steps.check-status.outputs.all_successful }}
      should_retry: ${{ steps.check-status.outputs.should_retry }}
    steps:
      - name: Check if all jobs succeeded
        id: check-status
        run: |
          echo "Checking job statuses..."
          
          # Check each job result
          SETUP_STATUS="${{ needs.setup.result }}"
          WCAG_TEST_STATUS="${{ needs.wcag-test.result }}"
          MERGE_SUMMARIES_STATUS="${{ needs.merge-summaries.result }}"
          
          echo "Setup job status: $SETUP_STATUS"
          echo "WCAG test job status: $WCAG_TEST_STATUS"
          echo "Merge summaries job status: $MERGE_SUMMARIES_STATUS"
          
          # Validate setup job outputs if setup succeeded
          if [[ "$SETUP_STATUS" == "success" ]]; then
            MATRIX_OUTPUT='${{ needs.setup.outputs.matrix }}'
            if [[ -z "$MATRIX_OUTPUT" || "$MATRIX_OUTPUT" == "null" ]]; then
              echo "‚ùå Setup job succeeded but produced invalid matrix output"
              SETUP_STATUS="failure"
            else
              echo "‚úÖ Setup job outputs validated"
            fi
          fi
          
          # Determine if all jobs were successful
          if [[ "$SETUP_STATUS" == "success" && "$WCAG_TEST_STATUS" == "success" && "$MERGE_SUMMARIES_STATUS" == "success" ]]; then
            echo "‚úÖ All jobs completed successfully"
            echo "all_successful=true" >> $GITHUB_OUTPUT
            echo "should_retry=false" >> $GITHUB_OUTPUT
          else
            echo "‚ùå One or more jobs failed:"
            echo "  Setup: $SETUP_STATUS"
            echo "  WCAG Test: $WCAG_TEST_STATUS"
            echo "  Merge Summaries: $MERGE_SUMMARIES_STATUS"
            
            # Check retry count
            RETRY_COUNT=${{ github.event.inputs.retry_count || '1' }}
            echo "Current retry attempt: $RETRY_COUNT"
            
            # Only retry if we haven't reached the maximum attempts and at least one job actually failed
            if [[ $RETRY_COUNT -lt 3 ]] && [[ \"$SETUP_STATUS\" == \"failure\" || \"$WCAG_TEST_STATUS\" == \"failure\" || \"$MERGE_SUMMARIES_STATUS\" == \"failure\" ]]; then
              echo "Will attempt retry (attempt $RETRY_COUNT of 3)"
              echo "should_retry=true" >> $GITHUB_OUTPUT
            else
              if [[ $RETRY_COUNT -ge 3 ]]; then
                echo "Maximum retry attempts (3) reached. Workflow will terminate."
              else
                echo "Jobs were cancelled or skipped, not retrying."
              fi
              echo "should_retry=false" >> $GITHUB_OUTPUT
            fi
            
            echo "all_successful=false" >> $GITHUB_OUTPUT
          fi

  retry-workflow:
    needs: [check-job-status]
    runs-on: self-hosted
    if: needs.check-job-status.outputs.should_retry == 'true'
    steps:
      - name: Wait before retry
        run: |
          RETRY_COUNT=${{ github.event.inputs.retry_count || '1' }}
          WAIT_TIME=$((RETRY_COUNT * 300))  # Exponential backoff: 5min, 10min, 15min
          echo "Waiting $WAIT_TIME seconds before retrying the workflow (attempt $RETRY_COUNT)..."
          sleep $WAIT_TIME
          
      - name: Trigger workflow retry
        uses: actions/github-script@v8
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const retryCount = parseInt('${{ github.event.inputs.retry_count || '1' }}') + 1;
            console.log(`Triggering retry attempt ${retryCount}`);
            
            await github.rest.actions.createWorkflowDispatch({
              owner: context.repo.owner,
              repo: context.repo.repo,
              workflow_id: 'deploy-selfhosted.yml',
              ref: 'main',
              inputs: {
                retry_count: retryCount.toString()
              }
            });
            
            console.log('Retry workflow triggered successfully');

  deploy:
    needs: [wcag-test, merge-summaries, check-job-status]
    runs-on: self-hosted
    if: needs.check-job-status.outputs.all_jobs_successful == 'true'
    steps:
      - name: Clean workspace
        run: |
          # Kill any hanging npm or node processes first
          pkill -f npm || echo "No npm processes to kill"
          pkill -f node || echo "No node processes to kill"
          
          # Wait for processes to fully terminate
          sleep 3
          
          # Force cleanup of problematic directories (try without sudo first)
          rm -rf node_modules || echo "Cleaned node_modules"
          rm -rf accessibility-reports || echo "Cleaned accessibility-reports"
          rm -rf playwright-report || echo "Cleaned playwright-report"
          rm -rf tests || echo "Cleaned tests"
          rm -rf .git/index.lock || echo "Cleaned git lock file"

      - name: Checkout repository
        uses: actions/checkout@v5
        with:
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Configure Git to use GitHub token
        run: |
          git config --global url."https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/".insteadOf "https://github.com/"
          git config --global user.email "github-actions@github.com"
          git config --global user.name "GitHub Actions"

      - name: Check if gh-pages branch exists
        id: check-branch
        run: |
          if git ls-remote --exit-code --heads origin gh-pages; then
            echo "exists=true" >> $GITHUB_OUTPUT
          else
            echo "exists=false" >> $GITHUB_OUTPUT
          fi

      - name: Create gh-pages branch if it doesn't exist
        if: steps.check-branch.outputs.exists == 'false'
        run: |
          echo "Creating gh-pages branch..."
          # Create orphan branch without checking it out
          git checkout --orphan temp-gh-pages
          git rm -rf .
          echo "Initial commit" > README.md
          git add README.md
          git commit -m "Initialize gh-pages branch"
          git push origin HEAD:gh-pages
          git checkout -f main

      - name: Checkout main branch
        uses: actions/checkout@v5
        with:
          ref: main
          path: main-branch

      - name: Checkout gh-pages branch
        uses: actions/checkout@v5
        with:
          ref: gh-pages
          path: gh-pages-branch

      - name: Download all artifacts
        uses: actions/download-artifact@v5
        with:
          path: artifacts
          pattern: test-results-batch-*
          merge-multiple: true
        continue-on-error: true

      - name: Check artifacts directory
        run: |
          echo "Checking artifacts directory..."
          if [ -d "artifacts" ]; then
            echo "Artifacts directory exists"
            ls -la artifacts/
          else
            echo "No artifacts directory found, creating empty one"
            mkdir -p artifacts
          fi

      - name: Download invalid URLs files
        uses: actions/download-artifact@v5
        with:
          name: invalid-urls-files
          path: invalid-urls-artifacts
        continue-on-error: true

      - name: Copy JSON Reports and Organize by Date
        run: |
          echo "Copying and organizing JSON reports to gh-pages branch..."
          mkdir -p gh-pages-branch/accessibility-reports
          CURRENT_DATE=$(date '+%Y-%m-%d')
          
          # Create date directory for current reports
          mkdir -p "gh-pages-branch/accessibility-reports/$CURRENT_DATE"

          # Debug: Show what's actually in artifacts
          echo "=== DEBUG: Contents of artifacts directory ==="
          find artifacts -type f -name "*.json" | head -20 || echo "No JSON files found"
          echo "=== DEBUG: Looking for violations files specifically ==="
          find artifacts -name "violations-*.json" | head -20 || echo "No violations files found"
          
          # Main search for accessibility report files - adjusted to fix the pathing issue
          FOUND_FILES=false
          
          # Search in nested accessibility-reports folder which is where the files actually are
          for VIOLATIONS_FILE in artifacts/accessibility-reports/violations-*.json; do
            if [ -f "$VIOLATIONS_FILE" ]; then
              FILENAME=$(basename "$VIOLATIONS_FILE")
              TARGET_FILE="gh-pages-branch/accessibility-reports/$CURRENT_DATE/$FILENAME"
              cp "$VIOLATIONS_FILE" "$TARGET_FILE"
              echo "Copied $VIOLATIONS_FILE to $TARGET_FILE"
              FOUND_FILES=true
            fi
          done
          
          # Try batch directories if the above doesn't work
          if [ "$FOUND_FILES" = false ]; then
            echo "Trying batch sub-directories..."
            for VIOLATIONS_FILE in artifacts/test-results-batch-*/accessibility-reports/violations-*.json; do
              if [ -f "$VIOLATIONS_FILE" ]; then
                FILENAME=$(basename "$VIOLATIONS_FILE")
                TARGET_FILE="gh-pages-branch/accessibility-reports/$CURRENT_DATE/$FILENAME"
                cp "$VIOLATIONS_FILE" "$TARGET_FILE"
                echo "Copied $VIOLATIONS_FILE to $TARGET_FILE"
                FOUND_FILES=true
              fi
            done
          fi
          
          # Try original paths as a last resort
          if [ "$FOUND_FILES" = false ]; then
            echo "No files found in accessibility-reports, trying direct artifacts search..."
            for VIOLATIONS_FILE in artifacts/violations-*.json; do
              if [ -f "$VIOLATIONS_FILE" ]; then
                FILENAME=$(basename "$VIOLATIONS_FILE")
                TARGET_FILE="gh-pages-branch/accessibility-reports/$CURRENT_DATE/$FILENAME"
                cp "$VIOLATIONS_FILE" "$TARGET_FILE"
                echo "Copied $VIOLATIONS_FILE to $TARGET_FILE"
                FOUND_FILES=true
              fi
            done
          fi
          
          if [ "$FOUND_FILES" = false ]; then
            echo "Warning: No violations files found to copy"
            # Create a placeholder file to ensure the directory exists with content
            echo '{"status":"no_violations_found","date":"'$CURRENT_DATE'"}' > "gh-pages-branch/accessibility-reports/$CURRENT_DATE/no-violations-found.json"
            echo "Created a placeholder file in $CURRENT_DATE directory"
          fi

          echo "JSON reports organized by date in gh-pages branch"
          echo "Contents of gh-pages-branch/accessibility-reports/$CURRENT_DATE:"
          ls -la "gh-pages-branch/accessibility-reports/$CURRENT_DATE/" || echo "No reports found"
          
          # CRITICAL: Ensure the date directory exists even if no reports
          # This prevents the "no folder" error on the frontend
          if [ ! -d "gh-pages-branch/accessibility-reports/$CURRENT_DATE" ] || [ -z "$(ls -A gh-pages-branch/accessibility-reports/$CURRENT_DATE 2>/dev/null)" ]; then
            echo "WARNING: No reports or empty directory for $CURRENT_DATE"
            mkdir -p "gh-pages-branch/accessibility-reports/$CURRENT_DATE"
            
            # Create a placeholder file to ensure directory has content
            echo "{
              \"date\": \"$CURRENT_DATE\",
              \"status\": \"no_violations_found\",
              \"message\": \"No accessibility violations were detected or tests did not complete\",
              \"timestamp\": \"$(date -Iseconds)\"
            }" > "gh-pages-branch/accessibility-reports/$CURRENT_DATE/placeholder.json"
            
            echo "Created placeholder for $CURRENT_DATE"
          fi

      - name: Generate Daily Summary Files
        run: |
          echo "Generating daily summary files from violation reports..."
          
          # Setup Node.js environment for tag summary generation
          cd main-branch
          npm install || echo "Dependencies already installed"
          
          # Copy tag-summary.js to current directory if not present
          if [ ! -f "tag-summary.js" ]; then
            echo "Error: tag-summary.js not found in main branch"
            exit 1
          fi
          
          CURRENT_DATE=$(date '+%Y-%m-%d')
          VIOLATIONS_DIR="../gh-pages-branch/accessibility-reports/$CURRENT_DATE"
          
          # Check if we have any violation files to process
          if [ ! -d "$VIOLATIONS_DIR" ]; then
            echo "No violations directory found: $VIOLATIONS_DIR"
            exit 0
          fi
          
          VIOLATION_FILES=$(find "$VIOLATIONS_DIR" -name "violations-*.json" | wc -l)
          echo "Found $VIOLATION_FILES violation files to process"
          
          if [ "$VIOLATION_FILES" -eq 0 ]; then
            echo "No violation files found, skipping summary generation"
            exit 0
          fi
            # Process each violation file to generate/update daily summary
          for VIOLATIONS_FILE in "$VIOLATIONS_DIR"/violations-*.json; do
            if [ -f "$VIOLATIONS_FILE" ]; then
              echo "Processing $VIOLATIONS_FILE for daily summary..."
          
              # Get absolute path for the violations file
              ABSOLUTE_VIOLATIONS_FILE=$(realpath "$VIOLATIONS_FILE")
              echo "Absolute path: $ABSOLUTIONS_VIOLATIONS_FILE"
          
              # Use Node.js to run the tag summary generation
              node -e "
                const { generateTagSummary } = require('./tag-summary.js');
                const fs = require('fs');
                const path = require('path');
          
                // Create absolute paths
                const violationsFile = '$ABSOLUTE_VIOLATIONS_FILE';
                const summaryDir = path.resolve('..', 'gh-pages-branch', 'accessibility-reports', 'summaries', '$CURRENT_DATE');
          
                console.log('Processing violations file:', violationsFile);
                console.log('Target summary directory:', summaryDir);
          
                // Ensure the summary directory exists in gh-pages-branch
                if (!fs.existsSync(summaryDir)) {
                  fs.mkdirSync(summaryDir, { recursive: true });
                  console.log('Created summary directory:', summaryDir);
                }
          
                // Generate the tag summary for this file
                try {
                  generateTagSummary(violationsFile);
                  console.log('Tag summary generation completed for:', violationsFile);
                } catch (error) {
                  console.error('Error generating tag summary:', error.message);
                }
          
                // Check if local summary was generated and copy it
                const localSummaryPath = path.join('accessibility-reports', 'summaries', '$CURRENT_DATE', 'daily_summary.json');
                const targetSummaryPath = path.join(summaryDir, 'daily_summary.json');
          
                if (fs.existsSync(localSummaryPath)) {
                  // Copy the summary to gh-pages branch
                  fs.copyFileSync(localSummaryPath, targetSummaryPath);
                  console.log('‚úì Copied daily summary to gh-pages branch:', targetSummaryPath);
                } else {
                  console.log('‚úó No local summary generated at:', localSummaryPath);
                  // List what files do exist in the summaries directory
                  const summariesParentDir = path.join('accessibility-reports', 'summaries', '$CURRENT_DATE');
                  if (fs.existsSync(summariesParentDir)) {
                    console.log('Files in summaries directory:', fs.readdirSync(summariesParentDir));
                  } else {
                    console.log('Summaries directory does not exist:', summariesParentDir);
                  }
                }
              "
            fi
          done
            # Verify the summary was created
          SUMMARY_PATH="../gh-pages-branch/accessibility-reports/summaries/$CURRENT_DATE/daily_summary.json"
          if [ -f "$SUMMARY_PATH" ]; then
            echo "‚úì Daily summary successfully generated at: $SUMMARY_PATH"
            echo "  Summary file size: $(stat -f%z "$SUMMARY_PATH" 2>/dev/null || stat -c%s "$SUMMARY_PATH" 2>/dev/null || echo "unknown") bytes"
            echo "  Summary preview (first 500 chars):"
            head -c 500 "$SUMMARY_PATH" 2>/dev/null || echo "  Could not preview summary file"
          else
            echo "‚úó Warning: Daily summary was not generated at expected path: $SUMMARY_PATH"
            echo "  Checking if summary directory exists..."
            SUMMARY_DIR="../gh-pages-branch/accessibility-reports/summaries/$CURRENT_DATE"
            if [ -d "$SUMMARY_DIR" ]; then
              echo "  Summary directory exists, contents:"
              ls -la "$SUMMARY_DIR/"
            else
              echo "  Summary directory does not exist: $SUMMARY_DIR"
            fi
          
            # Check if any summary files exist anywhere
            echo "  Searching for any daily_summary.json files:"
            find ../gh-pages-branch -name "daily_summary.json" 2>/dev/null || echo "  No daily summary files found anywhere"
          fi
          
          echo "Daily summary generation completed"

      - name: Copy Playwright Reports and Organize by Date
        run: |
          echo "Copying and organizing Playwright reports to gh-pages branch..."
          CURRENT_DATE=$(date '+%Y-%m-%d')
          TARGET_DIR="gh-pages-branch/playwright-reports/$CURRENT_DATE"
          mkdir -p "$TARGET_DIR"

          # Loop through all report directories for each project batch
          for REPORT_DIR in artifacts/test-results-batch-*/playwright-report; do
            echo "Checking directory: $REPORT_DIR"
            if [ -d "$REPORT_DIR" ]; then
              PROJECT_NAME=$(basename "$(dirname "$REPORT_DIR")")  # Extract project name
              PROJECT_DIR="$TARGET_DIR/$PROJECT_NAME"
              mkdir -p "$PROJECT_DIR"

              # Ensure the main report file is copied correctly
              if [ -f "$REPORT_DIR/index.html" ]; then
                cp "$REPORT_DIR/index.html" "$PROJECT_DIR/index.html"
                echo "Saved Playwright report for $PROJECT_NAME in $PROJECT_DIR/index.html"
              else
                echo "Warning: No index.html found in $REPORT_DIR"
              fi
            fi
          done

          echo "All HTML reports saved in $TARGET_DIR"

      - name: Create Gzip Archives for JSON Reports
        run: |
          echo "Creating gzip archives for JSON reports by date..."
          
          # Find all date directories in accessibility-reports
          find gh-pages-branch/accessibility-reports -maxdepth 1 -type d -name "[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]" | while read -r date_dir; do
            DATE=$(basename "$date_dir")
            echo "Creating gzip archive for date: $DATE"
            
            # Create temporary directory for organizing files
            TEMP_DIR="temp_$DATE"
            mkdir -p "$TEMP_DIR"
            
            # Copy only the newest JSON files per project for this date to temp directory
            if [ -d "$date_dir" ]; then
              # Find all violation JSON files and get the newest per project
              # This mimics the logic from getNewestReportsForDate() function
              
              # First, find all violation files and create a list
              find "$date_dir" -name "violations-*.json" ! -name "*-FAILED.json" | sort > "temp_violations_$DATE.txt"
              
              # Process each file to extract project and keep newest per project
              declare -A project_files
              while IFS= read -r file; do
                filename=$(basename "$file")
                
                # Extract project name (everything between violations- and the date)
                if [[ "$filename" =~ violations-(.+)-[0-9]{4}-[0-9]{2}-[0-9]{2} ]]; then
                  project="${BASH_REMATCH[1]}"
                  
                  # Store the file if it's newer than the current one for this project
                  current_file="${project_files[$project]}"
                  if [[ -z "$current_file" ]] || [[ "$filename" > "$(basename "$current_file")" ]]; then
                    project_files["$project"]="$file"
                  fi
                fi
              done < "temp_violations_$DATE.txt"
              
              # Copy the newest files for each project
              for file in "${project_files[@]}"; do
                if [[ -f "$file" ]]; then
                  cp "$file" "$TEMP_DIR/"
                fi
              done
              
              # Also copy any non-violation JSON files (like summary files, etc.)
              find "$date_dir" -name "*.json" ! -name "violations-*.json" -exec cp {} "$TEMP_DIR/" \;
              
              # Cleanup temp file
              rm -f "temp_violations_$DATE.txt"
              
              total_violations=$(find "$date_dir" -name "violations-*.json" ! -name "*-FAILED.json" | wc -l)
              selected_violations=$(ls "$TEMP_DIR"/violations-*.json 2>/dev/null | wc -l)
              echo "Found $total_violations total violation files, selected $selected_violations newest per project for $DATE"
            fi
            
            # Also include summary files for this date if they exist
            if [ -d "gh-pages-branch/accessibility-reports/summaries/$DATE" ]; then
              find "gh-pages-branch/accessibility-reports/summaries/$DATE" -name "*.json" -exec cp {} "$TEMP_DIR/" \;
              echo "Found $(find "gh-pages-branch/accessibility-reports/summaries/$DATE" -name "*.json" | wc -l) summary files for $DATE"
            fi
            
            # Create gzip archive if we have files
            if [ "$(find "$TEMP_DIR" -name "*.json" | wc -l)" -gt 0 ]; then
              # Create archive directory
              mkdir -p "gh-pages-branch/accessibility-reports/archives"
              
              # Create tar.gz archive containing all JSON files for this date
              # Use file list approach to avoid "Argument list too long" error
              cd "$TEMP_DIR"
              find . -name "*.json" > filelist.txt
              tar -czf "../gh-pages-branch/accessibility-reports/archives/reports_$DATE.tar.gz" -T filelist.txt
              cd ..
              
              # Get file size for logging
              ARCHIVE_SIZE=$(du -h "gh-pages-branch/accessibility-reports/archives/reports_$DATE.tar.gz" | cut -f1)
              echo "Created archive: reports_$DATE.tar.gz (Size: $ARCHIVE_SIZE)"
              
              # Create a manifest file with the list of files in the archive
              tar -tzf "gh-pages-branch/accessibility-reports/archives/reports_$DATE.tar.gz" | sort > "gh-pages-branch/accessibility-reports/archives/reports_$DATE.manifest"
              echo "Created manifest: reports_$DATE.manifest"
            else
              echo "No JSON files found for $DATE, skipping archive creation"
            fi
            
            # Clean up temp directory
            rm -rf "$TEMP_DIR"
          done
          
          # Create archive index with metadata
          echo "Creating archive index..."
          cd gh-pages-branch/accessibility-reports/archives
          ls -la *.tar.gz 2>/dev/null | while read -r permissions links owner group size month day time filename; do
            if [[ "$filename" =~ reports_([0-9]{4}-[0-9]{2}-[0-9]{2})\.tar\.gz ]]; then
              DATE="${BASH_REMATCH[1]}"
              FILE_COUNT=$(wc -l < "reports_$DATE.manifest" 2>/dev/null || echo "0")
              echo "{\"date\":\"$DATE\",\"filename\":\"$filename\",\"size\":\"$size\",\"fileCount\":$FILE_COUNT}"
            fi
          done | jq -s 'sort_by(.date) | reverse' > archive_index.json 2>/dev/null || echo "[]" > archive_index.json
          cd ../../..
          
          # Extract daily summaries for direct access
          echo "Extracting daily summaries for direct access..."
          find gh-pages-branch/accessibility-reports/archives -name "reports_*.tar.gz" | while read -r archive; do
            if [[ "$archive" =~ reports_([0-9]{4}-[0-9]{2}-[0-9]{2})\.tar\.gz ]]; then
              DATE="${BASH_REMATCH[1]}"
              echo "Extracting summary for $DATE from $(basename "$archive")"
              
              # Create summaries directory for this date
              mkdir -p "gh-pages-branch/accessibility-reports/summaries/$DATE"
              
              # Extract daily_summary.json if it exists in the archive
              if tar -tzf "$archive" | grep -q "daily_summary.json"; then
                tar -xzf "$archive" -C "gh-pages-branch/accessibility-reports/summaries/$DATE" ./daily_summary.json 2>/dev/null
                if [[ -f "gh-pages-branch/accessibility-reports/summaries/$DATE/daily_summary.json" ]]; then
                  echo "‚úÖ Extracted daily_summary.json for $DATE"
                else
                  echo "‚ö†Ô∏è  Failed to extract daily_summary.json for $DATE"
                fi
              else
                echo "‚ö†Ô∏è  No daily_summary.json found in archive for $DATE"
              fi
            fi
          done
          
          echo "Archive creation completed. Summary:"
          ls -la gh-pages-branch/accessibility-reports/archives/ || echo "No archives created"

      - name: Update Report List
        run: |
          echo "Updating report list in gh-pages branch..."
          mkdir -p gh-pages-branch/historical-data
          find gh-pages-branch/accessibility-reports -type f -name "violations-*.json" ! -name "*-FAILED.json" -print0 | \
          while IFS= read -r -d '' file; do
            relative_path=${file#gh-pages-branch/}
            echo "$relative_path"
          done | sort | jq -R . | jq -s . > gh-pages-branch/historical-data/report-list.json
          
          echo "Report list contents:"
          cat gh-pages-branch/historical-data/report-list.json
          echo "Number of reports: $(jq length gh-pages-branch/historical-data/report-list.json)"

      - name: Deploy static files to GitHub Pages
        run: |
          echo "Copying static files to gh-pages branch..."
          cp main-branch/index.html gh-pages-branch/
          cp main-branch/favicon.ico gh-pages-branch/
          cp main-branch/README.md gh-pages-branch/
          
          # Copy new JavaScript and CSS files
          cp main-branch/loading.js gh-pages-branch/
          cp main-branch/summary.js gh-pages-branch/
          cp main-branch/styles.css gh-pages-branch/
          cp main-branch/tag-summary.js gh-pages-branch/
          cp main-branch/invalid_urls.js gh-pages-branch/
          cp main-branch/merge-summary-parts.js gh-pages-branch/
          cp main-branch/validate-summaries.js gh-pages-branch/
          cp main-branch/gzip-utils.js gh-pages-branch/
          
          echo "Static files copied to gh-pages branch"
      
      - name: Pre-deployment Archive Verification
        run: |
          CURRENT_DATE=$(date +%Y-%m-%d)
          echo "Verifying archives before deployment..."
          
          if [ -f "gh-pages-branch/accessibility-reports/archives/reports_${CURRENT_DATE}.tar.gz" ]; then
            ARCHIVE_SIZE=$(stat -f%z "gh-pages-branch/accessibility-reports/archives/reports_${CURRENT_DATE}.tar.gz" 2>/dev/null || stat -c%s "gh-pages-branch/accessibility-reports/archives/reports_${CURRENT_DATE}.tar.gz" 2>/dev/null)
            echo "‚úÖ Archive exists: reports_${CURRENT_DATE}.tar.gz (Size: $ARCHIVE_SIZE bytes)"
            
            # Verify archive integrity
            tar -tzf "gh-pages-branch/accessibility-reports/archives/reports_${CURRENT_DATE}.tar.gz" > /dev/null 2>&1 && echo "‚úÖ Archive integrity verified" || echo "‚ùå Archive corrupted!"
          else
            echo "‚ùå ERROR: Archive missing before deployment: reports_${CURRENT_DATE}.tar.gz"
            ls -la gh-pages-branch/accessibility-reports/archives/ || echo "Archives directory doesn't exist"
          fi
      
      - name: Deploy to GitHub Pages (Direct Git Push)
        run: |
          echo "Deploying to gh-pages branch using direct git push..."
          CURRENT_DATE=$(date +%Y-%m-%d)
          
          cd gh-pages-branch
          
          # Initialize git repo
          git init
          git config user.name "GitHub Actions"
          git config user.email "actions@github.com"
          
          # Add all files with verbose output for archives
          git add -A
          
          # Show what's being committed (especially archives and summaries)
          echo "Files being deployed:"
          git status --short | grep -E "(archives|summaries|$CURRENT_DATE)" | head -30
          TOTAL_FILES=$(git status --short | wc -l)
          echo "Total files to commit: $TOTAL_FILES"
          
          # Specifically verify today's archive is staged
          if git status --short | grep -q "accessibility-reports/archives/reports_${CURRENT_DATE}.tar.gz"; then
            echo "‚úÖ Today's archive is staged for commit: reports_${CURRENT_DATE}.tar.gz"
          else
            echo "‚ö†Ô∏è WARNING: Today's archive not in git status!"
            # Force add it explicitly
            if [ -f "accessibility-reports/archives/reports_${CURRENT_DATE}.tar.gz" ]; then
              git add -f "accessibility-reports/archives/reports_${CURRENT_DATE}.tar.gz"
              git add -f "accessibility-reports/archives/reports_${CURRENT_DATE}.manifest"
              echo "Force-added archive files"
            fi
          fi
          
          # Specifically verify today's summaries are staged
          if git status --short | grep -q "accessibility-reports/summaries/$CURRENT_DATE"; then
            echo "‚úÖ Today's summaries are staged for commit"
          else
            echo "‚ö†Ô∏è WARNING: Today's summaries not in git status!"
            # Force add summaries directory explicitly
            if [ -d "accessibility-reports/summaries/$CURRENT_DATE" ]; then
              git add -f "accessibility-reports/summaries/$CURRENT_DATE/"
              echo "Force-added summary files for $CURRENT_DATE"
            else
              echo "‚ùå Summaries directory not found: accessibility-reports/summaries/$CURRENT_DATE"
            fi
          fi
          
          # Commit with descriptive message
          git commit -m "Deploy accessibility reports for $CURRENT_DATE [skip ci]" || {
            echo "No changes to commit"
            exit 0
          }
          
          # Setup remote - check if it exists first
          git branch -M gh-pages
          
          # Check if remote exists, if not add it
          if ! git remote get-url origin &>/dev/null; then
            git remote add origin https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git
          else
            # Update the remote URL to ensure it has the token
            git remote set-url origin https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git
          fi
          
          # Push with retry logic and verbose output
          MAX_RETRIES=3
          for i in $(seq 1 $MAX_RETRIES); do
            echo "Push attempt $i of $MAX_RETRIES..."
            if git push -f origin gh-pages 2>&1 | tail -20; then
              echo "‚úÖ Successfully pushed to gh-pages"
              break
            else
              echo "‚ö†Ô∏è Push attempt $i failed"
              if [ $i -lt $MAX_RETRIES ]; then
                echo "Retrying in 10 seconds..."
                sleep 10
              else
                echo "‚ùå All push attempts failed"
                exit 1
              fi
            fi
          done
          
          cd ..
          
      - name: Post-deployment Archive Verification
        run: |
          sleep 10
          CURRENT_DATE=$(date +%Y-%m-%d)
          
          echo "Verifying deployment of reports_${CURRENT_DATE}.tar.gz..."
          
          # Use curl to check for the file's existence via the GitHub API
          echo "Using curl to verify deployment..."
          API_URL="https://api.github.com/repos/${{ github.repository }}/contents/accessibility-reports/archives/reports_${CURRENT_DATE}.tar.gz?ref=gh-pages"
          RESPONSE=$(curl -s -o /dev/null -w "%{http_code}" -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" "$API_URL")
          
          # A status code of 200 means the file was found
          if [ "$RESPONSE" -eq 200 ]; then
            # Optional: Get the size of the file for a more detailed success message
            SIZE=$(curl -s -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" "$API_URL" | jq '.size' 2>/dev/null)
            echo "‚úÖ Archive successfully deployed: reports_${CURRENT_DATE}.tar.gz (Size: $SIZE bytes)"
          else
            # This is a critical failure. The push reported success but the file is not on the remote branch.
            echo "‚ùå CRITICAL: Archive not found after deployment!"
            echo "HTTP Status Code: $RESPONSE"
            echo "Expected URL: $API_URL"
            
            echo "Listing files in archives directory:"
            curl -s -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" \
                "https://api.github.com/repos/${{ github.repository }}/contents/accessibility-reports/archives?ref=gh-pages" | \
                jq '.[].name' 2>/dev/null || echo "Could not list archive files"
            
            # The job must fail if the critical file is not found
            exit 1
          fi
      
      - name: Trigger Issue Creation
        uses: actions/github-script@v8
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            await github.rest.actions.createWorkflowDispatch({
              owner: context.repo.owner,
              repo: context.repo.repo,
              workflow_id: 'create-update-issues-selfhosted.yml',
              ref: 'main'
            })

  workflow-failure-notification:
    needs: [setup, wcag-test, merge-summaries, check-job-status]
    runs-on: self-hosted
    if: always() && needs.check-job-status.outputs.all_jobs_successful == 'false' && needs.check-job-status.outputs.should_retry == 'false'
    steps:
      - name: Notify workflow failure
        run: |
          echo "üö® WORKFLOW FAILED AFTER MAXIMUM RETRIES"
          echo "The WCAG accessibility testing workflow has failed after 3 attempts."
          echo "Manual intervention may be required."
          echo ""
          echo "Failed jobs:"
          echo "  Setup: ${{ needs.setup.result }}"
          echo "  WCAG Test: ${{ needs.wcag-test.result }}"
          echo "  Merge Summaries: ${{ needs.merge-summaries.result }}"
          
          # Exit with error to mark workflow as failed
          exit 1
